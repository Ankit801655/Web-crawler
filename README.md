# Web Crawler for E-Commerce Product Links ğŸ•·ï¸ğŸ›†

## **Project Overview**
This project is a scalable **web crawler** built using **Scrapy + Playwright**, designed to extract product URLs from e-commerce websites. It supports **infinite scrolling, sitemap prioritization, BFS-based traversal with Redis**, and **robots.txt compliance**.

The backend is built using **Flask + PostgreSQL**, providing APIs to start/stop the crawler, fetch visited links, and retrieve product URLs. Authentication is implemented to protect access.

---

## **Features**
âœ… **Scalable BFS-based Web Crawler** using Scrapy + Redis  
âœ… **Infinite Scrolling Support** for dynamically loaded pages  
âœ… **Sitemap Prioritization** for efficient crawling  
âœ… **Robots.txt Compliance** to respect website policies  
âœ… **Flask API for Managing Crawling & Retrieving Data**  
âœ… **Authentication** for Secure API Access  
âœ… **PostgreSQL Integration** for storing crawled links  
âœ… **Error Handling & Logging** for robust performance  

---

## **Tech Stack**
- **Scrapy** (Crawling Framework)
- **Playwright** (Headless Browser for JS-rendered pages)
- **Redis** (BFS Queue + Visited URLs)
- **Flask** (API Layer)
- **PostgreSQL** (Database for storing URLs)
- **JWT Authentication** (API Security)

---

## **Project Structure**
```
Web-crawler/
â”‚â”€â”€ backend/                 # Flask Backend
â”‚   â”œâ”€â”€ app.py               # Flask main entry point
â”‚   â”œâ”€â”€ config.py            # Configuration settings
â”‚   â”œâ”€â”€ db.py                # Database initialization
â”‚   â”œâ”€â”€ models.py            # Database models (CrawledData, ProductLinks)
â”‚   â”œâ”€â”€ routes/              # API routes
â”‚   â”‚   â”œâ”€â”€ visited_links.py  # API for fetching visited links
â”‚   â”‚   â”œâ”€â”€ product_links.py  # API for fetching product links
â”‚â”€â”€ product_crawler/         # Scrapy Crawler
â”‚   â”œâ”€â”€ spiders/             # Scrapy spiders
â”‚   â”‚   â”œâ”€â”€ product_spider.py # Main crawling logic
â”‚   â”œâ”€â”€ utils/               # Utility scripts
â”‚   â”‚   â”œâ”€â”€ robots_handler.py # Robots.txt parsing
â”‚â”€â”€ migrations/              # Database migrations
â”‚â”€â”€ .venv/                   # Virtual environment (excluded from Git)
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ .gitignore               # Ignore unnecessary files
â”‚â”€â”€ README.md                # Project documentation
```

---

## **Setup Instructions**
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/your-username/web-crawler.git
cd web-crawler
```

### **2ï¸âƒ£ Create a Virtual Environment**
```bash
python -m venv .venv
```
Activate it:
- **Windows (CMD):** `.venv\Scripts\activate`
- **Windows (PowerShell):** `.venv\Scripts\Activate.ps1`
- **Linux/macOS:** `source .venv/bin/activate`

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **4ï¸âƒ£ Setup PostgreSQL**
Create a PostgreSQL database and user:
```sql
CREATE DATABASE web_crawler;
CREATE USER crawler_user WITH ENCRYPTED PASSWORD 'strong_password';
GRANT ALL PRIVILEGES ON DATABASE web_crawler TO crawler_user;
```
Update `backend/config.py` with the correct credentials.

### **5ï¸âƒ£ Initialize the Database**
```bash
flask db init
flask db migrate -m "Initial migration"
flask db upgrade
```

### **6ï¸âƒ£ Start Flask API**
```bash
flask run
```

---

## **Running the Crawler**
### **Start Crawler via API**
```bash
curl -X POST http://127.0.0.1:5000/start-crawler \
    -H "Authorization: Bearer YOUR_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{"domains": ["https://example.com", "https://webscraper.io"]}'
```
### **Stop Crawler via API**
```bash
curl -X POST http://127.0.0.1:5000/stop-crawler \
    -H "Authorization: Bearer YOUR_TOKEN"
```

---

## **API Endpoints**
| Method | Endpoint                  | Description |
|--------|---------------------------|-------------|
| `POST` | `/start-crawler`          | Start the Scrapy crawler |
| `POST` | `/stop-crawler`           | Stop the running crawler |
| `GET`  | `/visited-links`          | Retrieve all visited URLs |
| `GET`  | `/product-links`          | Retrieve extracted product URLs |
| `POST` | `/auth/login`             | Admin login to get JWT token |

#### **Example API Request for Authentication**
```bash
curl -X POST http://127.0.0.1:5000/auth/login \
     -H "Content-Type: application/json" \
     -d '{"username": "admin", "password": "password"}'
```
Expected Response:
```json
{
    "access_token": "your_generated_jwt_token"
}
```
Use this `access_token` in the `Authorization` header for all other API requests.

---

## **How Crawling Works**
1. **Sitemap Prioritization:** If a sitemap exists, those links are crawled first.
2. **BFS Traversal using Redis:** URLs are stored in Redis and dequeued in BFS order.
3. **robots.txt Compliance:** URLs are checked against robots.txt before crawling.
4. **Infinite Scroll Handling:** Pages with dynamic content are fully loaded.
5. **Product URL Filtering:** Product links are detected and stored separately.

---

## **Error Handling & Logging**
- Logs are stored in **scrapy_log.txt**.
- Errors are handled using try-except blocks across Flask and Scrapy.
- Playwright errors (timeouts, page crashes) are caught to prevent crashes.

---

## **Future Improvements**
ğŸš€ **Parallel Scrapy Workers** for better scalability  
ğŸš€ **Cloud Deployment** with Docker + AWS/GCP  
ğŸš€ **Machine Learning Model** for better product page detection  
ğŸš€ **Distributed Crawling** across multiple machines  

---

## **Contributing**
Want to improve this project? Follow these steps:
1. Fork the repository.
2. Create a feature branch (`git checkout -b feature-xyz`).
3. Commit your changes (`git commit -m "Added feature xyz"`).
4. Push to your branch (`git push origin feature-xyz`).
5. Open a pull request!

---

## **Contact**
For any queries, reach out via:
ğŸ“ Email: `ankitmajhi801@gmail.com`  
ğŸ™ GitHub: [Ankit801655](https://github.com/Ankit801655)  
ğŸš€ Happy Crawling! ğŸ¯

